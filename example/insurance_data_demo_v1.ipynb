{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ea4355",
   "metadata": {},
   "source": [
    "# Cyoda Client Demo\n",
    "\n",
    "This notebook demonstrates a basic data preprocessing example with a workflow for data cleaning.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Ensure you have the following before running the cells:\n",
    "\n",
    "- Cyoda API credentials (API key, secret, etc.)\n",
    "\n",
    "- Python packages installed via `requirements.txt`\n",
    "\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Setup**: Import necessary libraries and set up environment variables.\n",
    "\n",
    "2. **Authentication**: Authenticate with the Cyoda API.\n",
    "\n",
    "3. **Operations**: Perform basic and advanced operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7ffda",
   "metadata": {},
   "source": [
    "### Step 1: Setup Environment and Import Libraries\n",
    "Ensure you have set up your environment properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e85657",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "\n",
    "def setup_logging(level=logging.INFO):\n",
    "    logging.basicConfig(level=level)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    return logger\n",
    "\n",
    "logger = setup_logging()\n",
    "logger.info(\"Logging initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a474576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "API_KEY = os.environ[\"CYODA_API_KEY\"]\n",
    "API_SECRET = os.environ[\"CYODA_API_SECRET\"]\n",
    "API_URL = os.environ[\"CYODA_API_URL\"]+\"/api\"\n",
    "GRPC_ADDRESS = os.environ[\"GRPC_ADDRESS\"]\n",
    "WORK_DIR = os.environ[\"WORK_DIR\"]\n",
    "TOKEN = \"\"\n",
    "logger.info(f\"API URL: {API_URL}\")\n",
    "logger.info(f\"GRPC Address: {GRPC_ADDRESS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b18d177",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define entity and model version constants\n",
    "ENTITY_CLASS_NAME = \"com.cyoda.tdb.model.treenode.TreeNodeEntity\"\n",
    "ENTITY_NAME = \"insurance_v1\"\n",
    "ENTITY_PROCESSED_NAME = \"insurance_v1_processed\"\n",
    "MODEL_VERSION = \"1000\"\n",
    "\n",
    "logger.info(f\"Using entity: {ENTITY_CLASS_NAME}, model version: {MODEL_VERSION}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f64a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the auth first\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def authenticate(api_key, api_secret, api_url):\n",
    "    login_url = f\"{api_url}/auth/login\"\n",
    "    headers = {\"Content-Type\": \"application/json\", \"X-Requested-With\": \"XMLHttpRequest\"}\n",
    "    auth_data = {\"username\": api_key, \"password\": api_secret}\n",
    "\n",
    "    logger.info(\"Attempting to authenticate with Cyoda API.\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(login_url, headers=headers, data=json.dumps(auth_data), timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            token = response.json().get(\"token\")\n",
    "            logger.info(\"Authentication successful!\")\n",
    "            return token\n",
    "        else:\n",
    "            logger.error(f\"Authentication failed with {response}\")\n",
    "            return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "TOKEN = authenticate(API_KEY, API_SECRET, API_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's define several supplementary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_get_request(path):\n",
    "    url = f\"{API_URL}/{path}\"\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {TOKEN}\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_post_request(path, data):\n",
    "    url = f\"{API_URL}/{path}\"\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {TOKEN}\"}\n",
    "    response = requests.post(url, headers=headers, data=data)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_put_request(path, data, timeout):\n",
    "    url = f\"{API_URL}/{path}\"\n",
    "    headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {TOKEN}\"}\n",
    "    response = requests.put(url, headers=headers, data=data, timeout=timeout)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_delete_request(path):\n",
    "    url = f\"{API_URL}/{path}\"\n",
    "    headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {TOKEN}\"}\n",
    "    response = requests.delete(url, headers=headers)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_file(file_path: str) -> str:\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read file at {file_path}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the env\n",
    "Let's remove the data for the existing schema and the schema itself so we can start from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_entity_data(entity_name, version):\n",
    "    path = f\"entity/TREE/{entity_name}/{version}\"\n",
    "    try:\n",
    "        response = send_delete_request(path=path)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            logger.info(f\"Successfully deleted entity '{entity_name}' with version '{version}'. Response: {response}\")\n",
    "        else:\n",
    "            logger.error(f\"Failed to delete entity '{entity_name}' with version '{version}'. Response: {response}\")\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while deleting entity '{entity_name}' with version '{version}': {e}\")\n",
    "        return {'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = delete_entity_data(ENTITY_NAME, MODEL_VERSION)\n",
    "logger.info(f\"Delete response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_entity_schema(entity_name, version):\n",
    "    try:\n",
    "        path = f\"treeNode/model/{entity_name}/{version}\"\n",
    "        response = send_delete_request(path=path)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            logger.info(f\"Successfully deleted schema for entity '{entity_name}' with version '{version}'. Response: {response}\")\n",
    "        else:\n",
    "            logger.error(f\"Failed to delete schema for entity '{entity_name}' with version '{version}'. Status Code: {response}, Response: {response}\")\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while deleting schema for entity '{entity_name}' with version '{version}': {e}\")\n",
    "        return {'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = delete_entity_schema(ENTITY_NAME, MODEL_VERSION)\n",
    "logger.info(f\"Delete response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data\n",
    "To begin, we'll first create the schema. Once the schema is in place, we will proceed to lock it to ensure its integrity. After that, we can move on to ingesting the data from the file into the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_entity_schema(entity_name, version, data):\n",
    "    path = f\"treeNode/model/import/JSON/SAMPLE_DATA/{entity_name}/{version}\"\n",
    "    \n",
    "    try:\n",
    "        response = send_post_request(path=path, data=data)\n",
    "        if response.status_code == 200:\n",
    "            logger.info(f\"Successfully saved schema for entity '{entity_name}' with version '{version}'. Response: {response}\")\n",
    "        else:\n",
    "            logger.error(f\"Failed to save schema for entity '{entity_name}' with version '{version}'. Response: {response}\")\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while saving schema for entity '{entity_name}' with version '{version}': {e}\")\n",
    "        return {'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_save_schema(entity_name, file_path):\n",
    "    data = read_file(file_path)\n",
    "    response = save_entity_schema(\n",
    "        entity_name=entity_name, version=MODEL_VERSION, data=data\n",
    "    )\n",
    "    logger.info(f\"Response: {response}\")\n",
    "\n",
    "file_path_base = f\"{WORK_DIR}/example/resources/insurance_schema.json\"\n",
    "test_save_schema(ENTITY_NAME, file_path_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lock_entity_schema(entity_name, version, data):\n",
    "    path = f\"treeNode/model/{entity_name}/{version}/lock\"\n",
    "\n",
    "    try:\n",
    "        response = send_put_request(path=path, data=data, timeout=30)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            logger.info(f\"Successfully locked schema for entity '{entity_name}' with version '{version}'. Response: {response}\")\n",
    "        else:\n",
    "            logger.error(f\"Failed to lock schema for entity '{entity_name}' with version '{version}'. Response: {response}\")\n",
    "        \n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while locking schema for entity '{entity_name}' with version '{version}': {e}\")\n",
    "        return {'error': str(e)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lock_schema(entity_name):\n",
    "    try:\n",
    "        response = lock_entity_schema(entity_name=entity_name, version=MODEL_VERSION, data={})\n",
    "        logger.info(f\"Response: {response}\")    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while testing schema locking for entity '{entity_name}': {e}\")\n",
    "\n",
    "test_lock_schema(ENTITY_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_new_entity(entity_name, version, data):\n",
    "    path = f\"entity/JSON/TREE/{entity_name}/{version}\"\n",
    "    logger.info(f\"Saving new entity to path: {path}\")\n",
    "    \n",
    "    try:\n",
    "        response = send_post_request(path=path, data=data)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            logger.info(f\"Successfully saved new entity. Response: {response}\")\n",
    "        else:\n",
    "            logger.error(f\"Failed to save new entity. Response: {response}\")\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while saving new entity '{entity_name}' with version '{version}': {e}\")\n",
    "        return {'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_save_new_entity(entity_name, file_path):\n",
    "    \n",
    "    try:\n",
    "        data = read_file(file_path)\n",
    "        response = save_new_entity(entity_name=entity_name, version=MODEL_VERSION, data=data) \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while testing save_new_entity for '{entity_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "file_path_base = f\"{WORK_DIR}/example/resources/insurance_data.json\"\n",
    "response = test_save_new_entity(ENTITY_NAME, file_path_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json = response.json()\n",
    "if 'entityIds' in response_json[0]:\n",
    "    entity_id = response_json[0]['entityIds'][0]\n",
    "    logger.info(f\"Extracted entity ID: {entity_id}\")\n",
    "else:\n",
    "    logger.error(\"Response JSON does not contain 'entityIds'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_current_state(entity_id):\n",
    "    \n",
    "    path = f\"platform-api/entity-info/fetch/lazy?entityClass={ENTITY_CLASS_NAME}&entityId={entity_id}&columnPath=state\"\n",
    "    response = send_get_request(path=path)\n",
    "    logger.info(response.json())\n",
    "    return response\n",
    "get_entity_current_state(entity_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(model, version):\n",
    "    \n",
    "    path = f\"entity/TREE/{model}/{version}\"\n",
    "    response = send_get_request(path=path)\n",
    "    logger.info(response.json())\n",
    "    return response\n",
    "get_entities(ENTITY_NAME, MODEL_VERSION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_entity(uuid):\n",
    "    path = f\"entity/TREE/{uuid}\"\n",
    "    response = send_get_request(path=path)\n",
    "    logger.info(response.json())\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with data\n",
    "We are going to execute an example of basic data preproceesing. We will establish bi-directional grpc connection, save prizes entites, receive calculation request to perform data preparation and analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Handling missing values without 'inplace'\n",
    "def missing_values_processing(df):\n",
    "    df['claim_amount'] = df['claim_amount'].fillna(df['claim_amount'].median())\n",
    "    df['claim_status'] = df['claim_status'].fillna(df['claim_status'].mode()[0])\n",
    "    df['encoding'] = df['encoding'].fillna(df['encoding'].mode()[0])\n",
    "    return df\n",
    "\n",
    "# Scaling/Normalization Process\n",
    "def scaling_normalization_process(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    df[['premium_amount', 'claim_amount']] = scaler.fit_transform(df[['premium_amount', 'claim_amount']])\n",
    "    return df\n",
    "\n",
    "# Date Parsing Process\n",
    "def date_parsing_process(df):\n",
    "    df['policy_issue_date'] = pd.to_datetime(df['policy_issue_date'], errors='coerce')\n",
    "    return df\n",
    "\n",
    "# Character Encoding Process\n",
    "def character_encoding_process(df):\n",
    "    df['encoding'] = df['encoding'].apply(lambda x: 'utf-8' if x in ['ascii', 'iso-8859-1', 'utf-16'] else x)\n",
    "    return df\n",
    "\n",
    "# Data Entry Fixing Process\n",
    "def data_entry_fixing_process(df):\n",
    "    df['claim_status'] = df['claim_status'].str.lower()\n",
    "    return df\n",
    "\n",
    "\n",
    "process_dispatch = {\n",
    "        \"missing_values_processing\": missing_values_processing,\n",
    "        \"scaling_normalization_process\": scaling_normalization_process,\n",
    "        \"date_parsing_process\": date_parsing_process,\n",
    "        \"character_encoding_process\": character_encoding_process,\n",
    "        \"data_entry_fixing_process\": data_entry_fixing_process\n",
    "    }\n",
    "\n",
    "def clean_data(processing_step, raw_data):\n",
    "    input_data = raw_data.get(\"data\")\n",
    "    df = pd.read_json(StringIO(json.dumps(input_data)))\n",
    "    if processing_step in process_dispatch:\n",
    "        df = process_dispatch[processing_step](df)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown processing step: {processing_step}\")\n",
    "    processed_data = df.to_json(orient='records', date_format='iso')\n",
    "    output_data = {\"data\": json.loads(processed_data)}\n",
    "    \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the environment for the processed entities. We need to add schema for insurance_processed entity and lock it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_entity_data(ENTITY_PROCESSED_NAME, MODEL_VERSION)\n",
    "delete_entity_schema(ENTITY_PROCESSED_NAME, MODEL_VERSION)\n",
    "test_save_schema(ENTITY_PROCESSED_NAME, f\"{WORK_DIR}/example/resources/insurance_schema.json\")\n",
    "test_lock_schema(ENTITY_PROCESSED_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPPC client\n",
    "Let's establish a gRPC bi-directional streaming connection. The process will begin by sending a 'Join' event, and we expect to receive a 'Greet' event in response.\n",
    "\n",
    "Following this, we will save the 'Insurance' entity. This entity will undergo a step-by-step transition from the 'None' state to the 'Done' state, which will trigger an external processor. Once triggered, the external processor will send calculation requests, which will initiate data cleaning functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please import example/export_workflow_for_TreeNodeEntity_prizes.json workflow before you proceed :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install gRPC and tools\n",
    "!pip install grpcio grpcio-tools\n",
    "\n",
    "# Step 2: Compile proto files\n",
    "!python -m grpc_tools.protoc -I. --python_out=. --pyi_out=. --grpc_python_out=. cyoda-cloud-api.proto\n",
    "\n",
    "!python -m grpc_tools.protoc -I. --python_out=. --pyi_out=. --grpc_python_out=. cloudevents.proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from enum import Enum\n",
    "\n",
    "class CloudEventType(str, Enum):\n",
    "    BASE_EVENT = \"BaseEvent\"\n",
    "    CALCULATION_MEMBER_JOIN_EVENT = \"CalculationMemberJoinEvent\"\n",
    "    CALCULATION_MEMBER_GREET_EVENT = \"CalculationMemberGreetEvent\"\n",
    "    ENTITY_PROCESSOR_CALCULATION_REQUEST = \"EntityProcessorCalculationRequest\"\n",
    "    ENTITY_PROCESSOR_CALCULATION_RESPONSE = \"EntityProcessorCalculationResponse\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import uuid\n",
    "import json\n",
    "import asyncio\n",
    "from cloudevents_pb2 import CloudEvent\n",
    "import cloudevents_pb2_grpc as cloudevents_grpc\n",
    "import cyoda_cloud_api_pb2 as cyoda_cloud_pb2\n",
    "import cyoda_cloud_api_pb2_grpc as cyoda_cloud_grpc\n",
    "\n",
    "# Constants for event creation and configuration\n",
    "TAGS = [\"data_cleaning_workflow\"]\n",
    "OWNER = \"PLAY\"\n",
    "SPEC_VERSION = \"1.0\"\n",
    "SOURCE = \"SimpleSample\"\n",
    "JOIN_EVENT_TYPE = \"CalculationMemberJoinEvent\"\n",
    "NOTIFICATION_EVENT_TYPE = \"EntityProcessorCalculationResponse\"\n",
    "GREET_EVENT_TYPE = \"CalculationMemberGreetEvent\"\n",
    "EVENT_ID_FORMAT = \"{uuid}\"\n",
    "FILE_PATH = f\"{WORK_DIR}/example/resources/insurance_data.json\"\n",
    "\n",
    "def create_cloud_event(event_id: str, source: str, event_type: str, data: dict) -> CloudEvent:\n",
    "    \"\"\"\n",
    "    Create a CloudEvent instance with the given parameters.\n",
    "\n",
    "    :param event_id: Unique identifier for the event.\n",
    "    :param source: Source of the event.\n",
    "    :param event_type: Type of the event.\n",
    "    :param data: Data associated with the event.\n",
    "    :return: A CloudEvent instance.\n",
    "    \"\"\"\n",
    "    return CloudEvent(\n",
    "        id=event_id,\n",
    "        source=source,\n",
    "        spec_version=SPEC_VERSION,\n",
    "        type=event_type,\n",
    "        text_data=json.dumps(data)\n",
    "    )\n",
    "\n",
    "def create_join_event() -> CloudEvent:\n",
    "    \"\"\"\n",
    "    Create a CloudEvent for a member join event.\n",
    "\n",
    "    :return: A CloudEvent instance for the join event.\n",
    "    \"\"\"\n",
    "    return create_cloud_event(\n",
    "        event_id=str(uuid.uuid4()),\n",
    "        source=SOURCE,\n",
    "        event_type=JOIN_EVENT_TYPE,\n",
    "        data={\"owner\": OWNER, \"tags\": TAGS}\n",
    "    )\n",
    "\n",
    "def create_notification_event(data: dict) -> CloudEvent:\n",
    "    \"\"\"\n",
    "    Create a CloudEvent for a notification response.\n",
    "\n",
    "    :param data: Data from the notification response.\n",
    "    :return: A CloudEvent instance for the notification event.\n",
    "    \"\"\"\n",
    "    return create_cloud_event(\n",
    "        event_id=str(uuid.uuid4()),\n",
    "        source=SOURCE,\n",
    "        event_type=NOTIFICATION_EVENT_TYPE,\n",
    "        data={\n",
    "            \"requestId\": data.get('requestId'),\n",
    "            \"entityId\": data.get('entityId'),\n",
    "            \"owner\": OWNER,\n",
    "            \"payload\": data.get('payload'),\n",
    "            \"success\": True\n",
    "        }\n",
    "    )\n",
    "\n",
    "def trigger_entity_saved_event():\n",
    "    \"\"\"\n",
    "    Trigger an event when an entity is saved.\n",
    "    \"\"\"\n",
    "    test_save_new_entity(ENTITY_NAME, FILE_PATH)\n",
    "\n",
    "async def event_generator(queue: asyncio.Queue):\n",
    "    \"\"\"\n",
    "    Generate and yield events including initial and follow-up events.\n",
    "\n",
    "    :param queue: Async queue to get events from.\n",
    "    :yield: CloudEvent instances.\n",
    "    \"\"\"\n",
    "    # Yield the initial join event\n",
    "    yield create_join_event()\n",
    "    await asyncio.sleep(5)\n",
    "    \n",
    "    while True:\n",
    "        event = await queue.get()\n",
    "        if event is None:\n",
    "            break\n",
    "        yield event\n",
    "        queue.task_done()\n",
    "\n",
    "async def consume_stream():\n",
    "    \"\"\"\n",
    "    Handle bi-directional streaming with response-driven event generation.\n",
    "    \"\"\"\n",
    "    auth_creds = grpc.access_token_call_credentials(TOKEN)\n",
    "    credentials = grpc.composite_channel_credentials(grpc.ssl_channel_credentials(), auth_creds)\n",
    "    queue = asyncio.Queue()\n",
    "\n",
    "    async with grpc.aio.secure_channel(GRPC_ADDRESS, credentials) as channel:\n",
    "        stub = cyoda_cloud_grpc.CloudEventsServiceStub(channel)\n",
    "\n",
    "        # Start the streaming call and pass the generator\n",
    "        call = stub.startStreaming(event_generator(queue))\n",
    "\n",
    "        async for response in call:\n",
    "            if response.type == GREET_EVENT_TYPE:\n",
    "                trigger_entity_saved_event()\n",
    "            else:\n",
    "                data = json.loads(response.text_data)\n",
    "                logger.info(f\"Received response: {response}\")\n",
    "\n",
    "                # Process notification response and send notification_event\n",
    "                if data.get('processorName') in process_dispatch:\n",
    "                    logger.info(f\"Processing notification data: {data}\")\n",
    "                    #if this is the first transition, use the initial data attached to the insurance entity\n",
    "                    previous_version_id = data['payload']['data'].get('previous_version_id')\n",
    "                    if previous_version_id == \"0\":\n",
    "                        raw_data = data['payload']['data']\n",
    "                    else:\n",
    "                        #else retrieve the previous version of cleaned data\n",
    "                        raw_data = get_single_entity(previous_version_id).json().get(\"tree\", {})\n",
    "                    cleaned_data = clean_data(data.get('processorName'), raw_data)\n",
    "                    #save insurance_processed entity with the new cleaned data\n",
    "                    response = save_new_entity(ENTITY_PROCESSED_NAME, MODEL_VERSION, json.dumps(cleaned_data))\n",
    "                    #update the previous version of the insurance_processed entity id\n",
    "                    data['payload']['data']['previous_version_id'] = response.json()[0][\"entityIds\"][0]\n",
    "                    notification_event = create_notification_event(data)\n",
    "                    await queue.put(notification_event)                 \n",
    "                elif data.get('processorName') == \"finish_workflow\":\n",
    "                    notification_event = create_notification_event(data)\n",
    "                    await queue.put(notification_event)\n",
    "                    # Signal the end of the stream\n",
    "                    await queue.put(None)\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main function to run producer and consumer tasks.\"\"\"\n",
    "    try:\n",
    "        await asyncio.wait_for(consume_stream(), timeout=20)\n",
    "    except asyncio.TimeoutError:\n",
    "        logger.info(\"Stream operation timed out\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect to have two insurance entities: one saved during testing and the other saved via the gRPC client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_entities(ENTITY_NAME, MODEL_VERSION)\n",
    "formatted_json = json.dumps(response.json(), indent=4)\n",
    "logger.info(formatted_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect to have five insurance processed entities: a new version of data for each transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_entities(ENTITY_PROCESSED_NAME, MODEL_VERSION)\n",
    "formatted_json = json.dumps(response.json(), indent=4)\n",
    "logger.info(formatted_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
