    {
        "ai_endpoint": "cyoda",
        "question": "Please remember these files contents and reuse later: [{\"common/service/entity_service_interface.py\": \"from abc import ABC, abstractmethod\\nfrom typing import List, Any\\n\\nclass EntityService(ABC):\\n\\n    @abstractmethod\\n    async def get_item(self, token: str, entity_model: str, entity_version: str, id: str) -> Any:\\n        \\\"\\\"\\\"Retrieve a single item based on its ID.\\\"\\\"\\\"\\n        pass\\n\\n    @abstractmethod\\n    async def get_items(self, token: str, entity_model: str, entity_version: str) -> List[Any]:\\n        \\\"\\\"\\\"Retrieve multiple items based on their IDs.\\\"\\\"\\\"\\n        pass\\n\\n    @abstractmethod\\n    async def get_single_item_by_condition(self, token: str, entity_model: str, entity_version: str, condition: Any) -> List[Any]:\\n        \\\"\\\"\\\"Retrieve multiple items based on their IDs.\\\"\\\"\\\"\\n        pass\\n\\n    @abstractmethod\\n    async def get_items_by_condition(self, token: str, entity_model: str, entity_version: str, condition: Any) -> List[Any]:\\n        \\\"\\\"\\\"Retrieve multiple items based on their IDs.\\\"\\\"\\\"\\n        pass\\n\\n    @abstractmethod\\n    async def add_item(self, token: str, entity_model: str, entity_version: str, entity: Any) -> Any:\\n        \\\"\\\"\\\"Add a new item to the repository.\\\"\\\"\\\"\\n        pass\\n\\n    @abstractmethod\\n    async def update_item(self, token: str, entity_model: str, entity_version: str, id: str, entity: Any, meta: Any) -> Any:\\n        \\\"\\\"\\\"Update an existing item in the repository.\\\"\\\"\\\"\\n        pass\"}, {\"common/service/trino_service.py\": \"from common.config.config import CYODA_AI_URL\\nfrom common.util.utils import send_post_request\\n\\nasync def get_trino_schema_id_by_entity_name(entity_name: str):\\n    return \\\"2f303900-d8e3-11ef-a78b-ea51a4527ea1\\\"\\n\\n#runs sql to retrieve data\\nasync def run_sql_query(token, query):\\n    resp = await send_post_request(token, CYODA_AI_URL, \\\"api/v1/trino/run-query\\\", query)\\n    return resp[\\\"message\\\"]\"}, {\"common/ai/ai_assistant_service.py\": \"from abc import ABC, abstractmethod\\n\\nclass IAiAssistantService(ABC):\\n    @abstractmethod\\n    def init_chat(self, token: str, chat_id: str) -> dict:\\n        \\\"\\\"\\\"Initialize chat sessions for all AI endpoints\\n\\n        Args:\\n            token: Authentication token\\n            chat_id: Unique chat identifier\\n\\n        Returns:\\n            dict: Success status of initialization\\n        \\\"\\\"\\\"\\n        pass\\n\\n    @abstractmethod\\n    def ai_chat(self, token: str, chat_id: str, ai_endpoint: str, ai_question: str) -> dict:\\n        \\\"\\\"\\\"Send chat message to appropriate AI endpoint\\n\\n        Args:\\n            token: Authentication token\\n            chat_id: Unique chat identifier\\n            ai_endpoint: Target AI endpoint to route message to, enum ['random', 'trino'], trino is used for data retrieval, data aggregation\\n            ai_question: Chat message/question to send\\n\\n        Returns:\\n            dict: AI response message\\n\\n        Raises:\\n            ValueError: If ai_endpoint is not recognized\\n        \\\"\\\"\\\"\\n        pass\\n\\n\"}, {\"processor_instruction.txt\": \"import json\\nimport logging\\nimport os\\nimport asyncio\\n\\nfrom app_init.app_init import ai_service, entity_service\\nfrom common.config.config import ENTITY_VERSION, TRINO_AI_API\\nfrom common.service.trino_service import get_trino_schema_id_by_entity_name\\nfrom common.util.utils import read_json_file, parse_json\\n\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\n# Use meta to get token.\\n# 'data' is the current entity data. You can find the entity data model in {current_entity_name}.json file, which is available in your session context.\\n# Study the data model before writing any code.\\n# In the process function, you can work with the current entity data, analyze it, get other entities to which the current entity has references, and add new \\\"dependent\\\" entities.\\n\\nasync def process_name(meta, data):\\n    \\\"\\\"\\\"Example process function. This is where the core business logic goes.\\\"\\\"\\\"\\n    try:\\n        # Retrieve the entity based on its ID from the entity service.\\n        entity = await entity_service.get_item(meta[\\\"token\\\"], some_entity_name, ENTITY_VERSION, data[\\\"id\\\"])\\n\\n        # Process the entity and create a dependent entity.\\n        dependant_entity_data = data[\\\"dependent_entity_data\\\"]\\n\\n        # Add the dependent entity using the entity service.\\n        dependant_entity_id = await entity_service.add_item(\\n            meta[\\\"token\\\"], dependant_entity_name, ENTITY_VERSION, dependant_entity_data\\n        )\\n        data[\\\"dependent_entity\\\"] = {\\\"technical_id\\\": dependant_entity_id}\\n        #no need to update current entity - it will be done automatically, but dependant entities should be updated manually if necessary\\n        logger.info(\\\"Dependent entity added successfully.\\\")\\n    except Exception as e:\\n        logger.error(f\\\"Error in process_name: {e}\\\")\\n        raise\\n\\n\\nasync def data_aggregation_process_name(meta, data):\\n    \\\"\\\"\\\"Example of an asynchronous function to aggregate data.\\\"\\\"\\\"\\n    try:\\n        # Read the schema file that defines the structure for aggregated data.\\n        base_dir = os.path.abspath(os.path.join(__file__, \\\"../../../\\\"))\\n        aggregated_data_entity_path = os.path.join(base_dir, 'aggregated_data_entity', 'aggregated_data_entity.json')\\n        aggregated_data_entity_schema = await read_json_file(aggregated_data_entity_path)\\n\\n        # Make API call to AI service to generate aggregated data report based on schema.\\n        aggregated_data = await ai_service.ai_chat(\\n            token=meta[\\\"token\\\"],\\n            chat_id=await get_trino_schema_id_by_entity_name(\\\"response_data_entity\\\"),\\n            ai_endpoint=TRINO_AI_API,\\n            ai_question=f\\\"Could you please return json report based on this schema: {json.dumps(aggregated_data_entity_schema)}. Return only json\\\"\\n        )\\n\\n        # Parse and validate the returned JSON data.\\n        aggregated_data_entity_data = json.loads(parse_json(aggregated_data))\\n\\n        # Store the aggregated data entity and get its ID.\\n        aggregated_data_entity_id = await entity_service.add_item(\\n            meta[\\\"token\\\"], \\\"aggregated_data_entity\\\", ENTITY_VERSION, aggregated_data_entity_data\\n        )\\n\\n        logger.info(\\\"Aggregated data entity added successfully.\\\")\\n    except Exception as e:\\n        logger.error(f\\\"Error in data_aggregation_process_name: {e}\\\")\\n        raise\\n\\n# Test example to show how to handle async function calls in unit tests:\\n\\nclass TestSendEmailProcess(unittest.TestCase):\\n\\n    @patch(\\\"app_init.app_init.entity_service.add_item\\\")\\n    @patch(\\\"app_init.app_init.ai_service.ai_chat\\\")\\n    def test_send_email_process(self, mock_ai_chat, mock_entity_service):\\n        # Arrange: mock dependencies\\n        mock_ai_chat.return_value = '{\\\"aggregated_data\\\": \\\"dummy_aggregated_data\\\"}'\\n        mock_entity_service.return_value = \\\"aggregated_data_entity_id\\\"\\n\\n        meta = {\\\"token\\\": \\\"test_token\\\"}\\n        data = {}\\n\\n        # Act: Execute the async function using asyncio.run()\\n        asyncio.run(data_aggregation_process_name(meta, data))\\n\\n        # Assert: Verify that the correct methods were called with the expected arguments\\n        mock_entity_service.assert_called_once_with(\\n            meta[\\\"token\\\"], \\\"aggregated_data_entity\\\", ENTITY_VERSION, {\\\"aggregated_data\\\": \\\"dummy_aggregated_data\\\"}\\n        )\\n\\n# Uncomment the line below to run the tests when this script is executed directly\\nif __name__ == '__main__':\\n    unittest.main()\\n\"}, {\"/tmp/e2ab4ff3-d90d-11ef-abd0-40c2ba0ac9eb/quart-client-template/entity/app_design_prd.md\": \"# Product Requirements Document (PRD) for Cyoda Design\\n\\n## Introduction\\n\\nThis document provides an overview of the Cyoda-based application designed to manage data ingestion, aggregation, report generation, and email notification. It explains how the Cyoda design aligns with the specified requirements, focusing on the structure of entities, workflows, and the event-driven architecture that powers the application. The design is represented in a Cyoda JSON format which is translated into a human-readable document for clarity.\\n\\n## What is Cyoda?\\n\\nCyoda is a serverless, event-driven framework that facilitates the management of workflows through entities representing jobs and data. Each entity has a defined state, and transitions between states are governed by events that occur within the system\\u2014enabling a responsive and scalable architecture.\\n\\n### Cyoda Entity Database\\n\\nIn the Cyoda ecosystem, entities are fundamental components that represent processes and data. The Cyoda design JSON outlines several entities for our application:\\n\\n1. **Data Ingestion Job (`data_ingestion_job`)**:\\n   - **Type**: JOB\\n   - **Source**: SCHEDULED\\n   - **Description**: This job is responsible for ingesting data from specified sources at scheduled intervals (once a day).\\n\\n2. **Raw Data Entity (`raw_data_entity`)**:\\n   - **Type**: EXTERNAL_SOURCES_PULL_BASED_RAW_DATA\\n   - **Source**: ENTITY_EVENT\\n   - **Description**: This entity stores the raw data that has been ingested by the data ingestion job.\\n\\n3. **Aggregated Data Entity (`aggregated_data_entity`)**:\\n   - **Type**: SECONDARY_DATA\\n   - **Source**: ENTITY_EVENT\\n   - **Description**: This entity holds the aggregated data derived from the raw data for reporting purposes.\\n\\n4. **Report Entity (`report_entity`)**:\\n   - **Type**: SECONDARY_DATA\\n   - **Source**: ENTITY_EVENT\\n   - **Description**: This entity contains the generated report that is sent to the admin via email.\\n\\n### Workflow Overview\\n\\nThe workflows in Cyoda define the processes tied to each job entity. They consist of transitions that specify how an entity changes its state as events occur. The `data_ingestion_job` includes the following transitions:\\n\\n- **Scheduled Data Ingestion**: This transition starts the data ingestion process based on a schedule.\\n- **Aggregate Data**: After ingestion, this transition aggregates the raw data.\\n- **Generate and Send Report**: Finally, this transition creates a report from the aggregated data and sends it to the admin's email.\\n\\n### Event-Driven Approach\\n\\nAn event-driven architecture allows the application to respond automatically to changes or triggers. In this specific requirement, the following events occur:\\n\\n1. **Data Ingestion**: The data ingestion job is triggered on a scheduled basis, automatically initiating the process of fetching data.\\n2. **Data Aggregation**: Once data ingestion is complete, an event signals the need to aggregate the ingested data.\\n3. **Report Generation and Sending**: After the aggregation is complete, another event triggers the report generation and subsequent emailing to the admin.\\n\\nThis approach promotes scalability and efficiency by allowing the application to handle each process step automatically without manual intervention.\\n\\n### Sequence Diagram\\n\\n```mermaid\\nsequenceDiagram\\n    participant User\\n    participant Scheduler\\n    participant Data Ingestion Job\\n    participant Raw Data Entity\\n    participant Aggregated Data Entity\\n    participant Report Entity\\n    participant Admin\\n\\n    User->>Scheduler: Schedule data ingestion job\\n    Scheduler->>Data Ingestion Job: Trigger scheduled_data_ingestion\\n    Data Ingestion Job->>Raw Data Entity: Ingest data\\n    Raw Data Entity-->>Data Ingestion Job: Data ingested\\n    Data Ingestion Job->>Aggregated Data Entity: Aggregate data\\n    Aggregated Data Entity-->>Data Ingestion Job: Data aggregated\\n    Data Ingestion Job->>Report Entity: Generate report\\n    Report Entity-->>Data Ingestion Job: Report generated\\n    Data Ingestion Job->>Admin: Send report\\n```\\n\\n### Actors Involved\\n\\n- **User**: Initiates the scheduling of the data ingestion job.\\n- **Scheduler**: Responsible for triggering the job at predefined times.\\n- **Data Ingestion Job**: Central to managing the workflow of data processing.\\n- **Raw Data Entity**: Stores the ingested raw data.\\n- **Aggregated Data Entity**: Holds the aggregated data.\\n- **Report Entity**: Contains the generated report.\\n- **Admin**: Receives the report via email.\\n\\n## Conclusion\\n\\nThe Cyoda design aligns effectively with the requirements for creating a robust data processing application. By utilizing the event-driven model, the application can efficiently manage state transitions of each entity involved, from data ingestion to report delivery. The outlined entities, workflows, and events comprehensively cover the needs of the application, ensuring a smooth and automated process.\\n\\nThis PRD serves as a foundation for implementation and development, guiding the technical team through the specifics of the Cyoda architecture while providing clarity for users who may be new to the Cyoda framework.\"}, {\"/tmp/e2ab4ff3-d90d-11ef-abd0-40c2ba0ac9eb/quart-client-template/entity/aggregated_data_entity/aggregated_data_entity.json\": \"{\\n    \\\"id\\\": 1,\\n    \\\"total_activities\\\": 2,\\n    \\\"completed_activities\\\": 1,\\n    \\\"pending_activities\\\": 1,\\n    \\\"activity_summary\\\": [\\n        {\\n            \\\"activity_id\\\": 1,\\n            \\\"title\\\": \\\"Activity 1\\\",\\n            \\\"due_date\\\": \\\"2025-01-22T21:36:27.6587562+00:00\\\",\\n            \\\"status\\\": \\\"Pending\\\"\\n        },\\n        {\\n            \\\"activity_id\\\": 2,\\n            \\\"title\\\": \\\"Activity 2\\\",\\n            \\\"due_date\\\": \\\"2025-01-22T22:36:27.6587592+00:00\\\",\\n            \\\"status\\\": \\\"Completed\\\"\\n        }\\n    ],\\n    \\\"overall_status\\\": \\\"Partially Completed\\\",\\n    \\\"aggregation_timestamp\\\": \\\"2023-10-01T10:00:00Z\\\",\\n    \\\"comments\\\": \\\"This aggregated data reflects the current status of activities processed within the specified period.\\\"\\n}\"}, {\"/tmp/e2ab4ff3-d90d-11ef-abd0-40c2ba0ac9eb/quart-client-template/entity/report_entity/report_entity.json\": \"{\\n    \\\"report_id\\\": \\\"report_2023_10_01\\\",\\n    \\\"generated_at\\\": \\\"2023-10-01T10:05:00Z\\\",\\n    \\\"report_title\\\": \\\"Monthly Data Overview\\\",\\n    \\\"total_entries\\\": 150,\\n    \\\"successful_ingests\\\": 145,\\n    \\\"failed_ingests\\\": 5,\\n    \\\"percentage_success\\\": 96.67,\\n    \\\"activities_summary\\\": [\\n        {\\n            \\\"activity_id\\\": 1,\\n            \\\"title\\\": \\\"Activity 1\\\",\\n            \\\"due_date\\\": \\\"2025-01-22T21:36:27.6587562+00:00\\\",\\n            \\\"status\\\": \\\"Pending\\\"\\n        },\\n        {\\n            \\\"activity_id\\\": 2,\\n            \\\"title\\\": \\\"Activity 2\\\",\\n            \\\"due_date\\\": \\\"2025-01-22T22:36:27.6587592+00:00\\\",\\n            \\\"status\\\": \\\"Completed\\\"\\n        }\\n    ],\\n    \\\"overall_status\\\": \\\"Partially Completed\\\",\\n    \\\"comments\\\": \\\"This report summarizes the data ingestion activities for the month. It highlights the number of successful and failed ingests, along with details on specific activities processed.\\\",\\n    \\\"distribution_info\\\": {\\n        \\\"recipients\\\": [\\n            {\\n                \\\"name\\\": \\\"Admin User\\\",\\n                \\\"email\\\": \\\"admin@example.com\\\"\\n            },\\n            {\\n                \\\"name\\\": \\\"Data Analyst\\\",\\n                \\\"email\\\": \\\"analyst@example.com\\\"\\n            }\\n        ],\\n        \\\"sent_at\\\": \\\"2023-10-01T10:05:00Z\\\"\\n    }\\n}\"}, {\"/tmp/e2ab4ff3-d90d-11ef-abd0-40c2ba0ac9eb/quart-client-template/entity/app_design.json\": \"{\\n    \\\"$schema\\\": \\\"http://json-schema.org/draft-07/schema#\\\",\\n    \\\"title\\\": \\\"Cyoda design\\\",\\n    \\\"type\\\": \\\"object\\\",\\n    \\\"entities\\\": [\\n        {\\n            \\\"entity_name\\\": \\\"data_ingestion_job\\\",\\n            \\\"entity_type\\\": \\\"JOB\\\",\\n            \\\"entity_source\\\": \\\"SCHEDULED\\\",\\n            \\\"depends_on_entity\\\": \\\"None\\\",\\n            \\\"entity_workflow\\\": {\\n                \\\"name\\\": \\\"data_ingestion_workflow\\\",\\n                \\\"class_name\\\": \\\"com.cyoda.tdb.model.treenode.TreeNodeEntity\\\",\\n                \\\"transitions\\\": [\\n                    {\\n                        \\\"name\\\": \\\"scheduled_data_ingestion\\\",\\n                        \\\"description\\\": \\\"Triggered by a scheduled job to ingest data.\\\",\\n                        \\\"start_state\\\": \\\"None\\\",\\n                        \\\"start_state_description\\\": \\\"Initial state before data ingestion.\\\",\\n                        \\\"end_state\\\": \\\"data_ingested\\\",\\n                        \\\"end_state_description\\\": \\\"Data has been successfully ingested.\\\",\\n                        \\\"process\\\": {\\n                            \\\"name\\\": \\\"ingest_raw_data\\\",\\n                            \\\"description\\\": \\\"Process to ingest raw data from the specified data source.\\\",\\n                            \\\"adds_new_entites\\\": \\\"raw_data_entity\\\"\\n                        }\\n                    },\\n                    {\\n                        \\\"name\\\": \\\"aggregate_data\\\",\\n                        \\\"description\\\": \\\"Aggregate the ingested raw data.\\\",\\n                        \\\"start_state\\\": \\\"data_ingested\\\",\\n                        \\\"start_state_description\\\": \\\"Data has been ingested and is ready for aggregation.\\\",\\n                        \\\"end_state\\\": \\\"data_aggregated\\\",\\n                        \\\"end_state_description\\\": \\\"Data has been successfully aggregated.\\\",\\n                        \\\"process\\\": {\\n                            \\\"name\\\": \\\"aggregate_raw_data_process\\\",\\n                            \\\"description\\\": \\\"Process to aggregate raw data for reporting.\\\",\\n                            \\\"adds_new_entites\\\": \\\"aggregated_data_entity\\\"\\n                        }\\n                    },\\n                    {\\n                        \\\"name\\\": \\\"generate_and_send_report\\\",\\n                        \\\"description\\\": \\\"Generate a report and send it to admin via email.\\\",\\n                        \\\"start_state\\\": \\\"data_aggregated\\\",\\n                        \\\"start_state_description\\\": \\\"Data has been aggregated and is ready to be reported.\\\",\\n                        \\\"end_state\\\": \\\"report_sent\\\",\\n                        \\\"end_state_description\\\": \\\"Report has been successfully sent to admin.\\\",\\n                        \\\"process\\\": {\\n                            \\\"name\\\": \\\"generate_report_process\\\",\\n                            \\\"description\\\": \\\"Process to create a report and send it via email.\\\",\\n                            \\\"adds_new_entites\\\": \\\"report_entity\\\"\\n                        }\\n                    }\\n                ]\\n            }\\n        },\\n        {\\n            \\\"entity_name\\\": \\\"raw_data_entity\\\",\\n            \\\"entity_type\\\": \\\"EXTERNAL_SOURCES_PULL_BASED_RAW_DATA\\\",\\n            \\\"entity_source\\\": \\\"ENTITY_EVENT\\\",\\n            \\\"depends_on_entity\\\": \\\"data_ingestion_job\\\",\\n            \\\"entity_workflow\\\": {\\n                \\\"name\\\": \\\"raw_data_workflow\\\",\\n                \\\"class_name\\\": \\\"com.cyoda.tdb.model.treenode.TreeNodeEntity\\\",\\n                \\\"transitions\\\": []\\n            }\\n        },\\n        {\\n            \\\"entity_name\\\": \\\"aggregated_data_entity\\\",\\n            \\\"entity_type\\\": \\\"SECONDARY_DATA\\\",\\n            \\\"entity_source\\\": \\\"ENTITY_EVENT\\\",\\n            \\\"depends_on_entity\\\": \\\"data_ingestion_job\\\",\\n            \\\"entity_workflow\\\": {\\n                \\\"name\\\": \\\"aggregated_data_workflow\\\",\\n                \\\"class_name\\\": \\\"com.cyoda.tdb.model.treenode.TreeNodeEntity\\\",\\n                \\\"transitions\\\": []\\n            }\\n        },\\n        {\\n            \\\"entity_name\\\": \\\"report_entity\\\",\\n            \\\"entity_type\\\": \\\"SECONDARY_DATA\\\",\\n            \\\"entity_source\\\": \\\"ENTITY_EVENT\\\",\\n            \\\"depends_on_entity\\\": \\\"data_ingestion_job\\\",\\n            \\\"entity_workflow\\\": {\\n                \\\"name\\\": \\\"report_workflow\\\",\\n                \\\"class_name\\\": \\\"com.cyoda.tdb.model.treenode.TreeNodeEntity\\\",\\n                \\\"transitions\\\": []\\n            }\\n        }\\n    ]\\n}\"}, {\"/tmp/e2ab4ff3-d90d-11ef-abd0-40c2ba0ac9eb/quart-client-template/entity/data_ingestion_job/workflow/workflow.json\": \"{\\n    \\\"name\\\": \\\"data_ingestion_workflow\\\",\\n    \\\"description\\\": \\\"Workflow for ingesting, aggregating, and reporting data.\\\",\\n    \\\"workflow_criteria\\\": {\\n        \\\"externalized_criteria\\\": [],\\n        \\\"condition_criteria\\\": [\\n            {\\n                \\\"name\\\": \\\"data_ingestion_job\\\",\\n                \\\"description\\\": \\\"Workflow criteria\\\",\\n                \\\"condition\\\": {\\n                    \\\"group_condition_operator\\\": \\\"AND\\\",\\n                    \\\"conditions\\\": [\\n                        {\\n                            \\\"field_name\\\": \\\"entityModelName\\\",\\n                            \\\"is_meta_field\\\": true,\\n                            \\\"operation\\\": \\\"equals\\\",\\n                            \\\"value\\\": \\\"data_ingestion_job\\\",\\n                            \\\"value_type\\\": \\\"strings\\\"\\n                        }\\n                    ]\\n                }\\n            }\\n        ]\\n    },\\n    \\\"transitions\\\": [\\n        {\\n            \\\"name\\\": \\\"scheduled_data_ingestion\\\",\\n            \\\"description\\\": \\\"Triggered by a scheduled job to ingest data.\\\",\\n            \\\"start_state\\\": \\\"None\\\",\\n            \\\"start_state_description\\\": \\\"Initial state before data ingestion.\\\",\\n            \\\"end_state\\\": \\\"data_ingested\\\",\\n            \\\"end_state_description\\\": \\\"Data has been successfully ingested.\\\",\\n            \\\"automated\\\": true,\\n            \\\"transition_criteria\\\": {\\n                \\\"externalized_criteria\\\": [],\\n                \\\"condition_criteria\\\": []\\n            },\\n            \\\"processes\\\": {\\n                \\\"schedule_transition_processors\\\": [],\\n                \\\"externalized_processors\\\": [\\n                    {\\n                        \\\"name\\\": \\\"ingest_raw_data\\\",\\n                        \\\"description\\\": \\\"Process to ingest raw data from the specified data source.\\\",\\n                        \\\"calculation_nodes_tags\\\": \\\"e2ab4ff3-d90d-11ef-abd0-40c2ba0ac9eb\\\",\\n                        \\\"attach_entity\\\": true,\\n                        \\\"calculation_response_timeout_ms\\\": \\\"120000\\\",\\n                        \\\"retry_policy\\\": \\\"NONE\\\",\\n                        \\\"sync_process\\\": false,\\n                        \\\"new_transaction_for_async\\\": false,\\n                        \\\"none_transactional_for_async\\\": false,\\n                        \\\"processor_criteria\\\": {\\n                            \\\"externalized_criteria\\\": [],\\n                            \\\"condition_criteria\\\": []\\n                        }\\n                    }\\n                ]\\n            }\\n        },\\n        {\\n            \\\"name\\\": \\\"aggregate_data\\\",\\n            \\\"description\\\": \\\"Aggregate the ingested raw data.\\\",\\n            \\\"start_state\\\": \\\"data_ingested\\\",\\n            \\\"start_state_description\\\": \\\"Data has been ingested and is ready for aggregation.\\\",\\n            \\\"end_state\\\": \\\"data_aggregated\\\",\\n            \\\"end_state_description\\\": \\\"Data has been successfully aggregated.\\\",\\n            \\\"automated\\\": true,\\n            \\\"transition_criteria\\\": {\\n                \\\"externalized_criteria\\\": [],\\n                \\\"condition_criteria\\\": []\\n            },\\n            \\\"processes\\\": {\\n                \\\"schedule_transition_processors\\\": [],\\n                \\\"externalized_processors\\\": [\\n                    {\\n                        \\\"name\\\": \\\"aggregate_raw_data_process\\\",\\n                        \\\"description\\\": \\\"Process to aggregate raw data for reporting.\\\",\\n                        \\\"calculation_nodes_tags\\\": \\\"e2ab4ff3-d90d-11ef-abd0-40c2ba0ac9eb\\\",\\n                        \\\"attach_entity\\\": true,\\n                        \\\"calculation_response_timeout_ms\\\": \\\"120000\\\",\\n                        \\\"retry_policy\\\": \\\"NONE\\\",\\n                        \\\"sync_process\\\": false,\\n                        \\\"new_transaction_for_async\\\": false,\\n                        \\\"none_transactional_for_async\\\": false,\\n                        \\\"processor_criteria\\\": {\\n                            \\\"externalized_criteria\\\": [],\\n                            \\\"condition_criteria\\\": []\\n                        }\\n                    }\\n                ]\\n            }\\n        },\\n        {\\n            \\\"name\\\": \\\"generate_and_send_report\\\",\\n            \\\"description\\\": \\\"Generate a report and send it to admin via email.\\\",\\n            \\\"start_state\\\": \\\"data_aggregated\\\",\\n            \\\"start_state_description\\\": \\\"Data has been aggregated and is ready to be reported.\\\",\\n            \\\"end_state\\\": \\\"report_sent\\\",\\n            \\\"end_state_description\\\": \\\"Report has been successfully sent to admin.\\\",\\n            \\\"automated\\\": true,\\n            \\\"transition_criteria\\\": {\\n                \\\"externalized_criteria\\\": [],\\n                \\\"condition_criteria\\\": []\\n            },\\n            \\\"processes\\\": {\\n                \\\"schedule_transition_processors\\\": [],\\n                \\\"externalized_processors\\\": [\\n                    {\\n                        \\\"name\\\": \\\"generate_report_process\\\",\\n                        \\\"description\\\": \\\"Process to create a report and send it via email.\\\",\\n                        \\\"calculation_nodes_tags\\\": \\\"e2ab4ff3-d90d-11ef-abd0-40c2ba0ac9eb\\\",\\n                        \\\"attach_entity\\\": true,\\n                        \\\"calculation_response_timeout_ms\\\": \\\"120000\\\",\\n                        \\\"retry_policy\\\": \\\"NONE\\\",\\n                        \\\"sync_process\\\": false,\\n                        \\\"new_transaction_for_async\\\": false,\\n                        \\\"none_transactional_for_async\\\": false,\\n                        \\\"processor_criteria\\\": {\\n                            \\\"externalized_criteria\\\": [],\\n                            \\\"condition_criteria\\\": []\\n                        }\\n                    }\\n                ]\\n            }\\n        }\\n    ]\\n}\"}, {\"/tmp/e2ab4ff3-d90d-11ef-abd0-40c2ba0ac9eb/quart-client-template/entity/data_ingestion_job/workflow/__init__.py\": \"\"}, {\"/tmp/e2ab4ff3-d90d-11ef-abd0-40c2ba0ac9eb/quart-client-template/entity/data_ingestion_job/__init__.py\": \"\"}, {\"/tmp/e2ab4ff3-d90d-11ef-abd0-40c2ba0ac9eb/quart-client-template/entity/data_ingestion_job/data_ingestion_job.json\": \"{\\n    \\\"job_id\\\": \\\"job_001\\\",\\n    \\\"job_name\\\": \\\"Daily Data Ingestion Job\\\",\\n    \\\"scheduled_time\\\": \\\"2023-10-01T05:00:00Z\\\",\\n    \\\"status\\\": \\\"completed\\\",\\n    \\\"start_time\\\": \\\"2023-10-01T05:00:00Z\\\",\\n    \\\"end_time\\\": \\\"2023-10-01T05:05:00Z\\\",\\n    \\\"total_records_processed\\\": 100,\\n    \\\"successful_records\\\": 95,\\n    \\\"failed_records\\\": 5,\\n    \\\"failure_reason\\\": [\\n        \\\"Timeout while fetching data\\\",\\n        \\\"API limit reached\\\"\\n    ],\\n    \\\"raw_data_entity\\\": {\\n        \\\"technical_id\\\": \\\"raw_data_entity_001\\\",\\n        \\\"records\\\": [\\n            {\\n                \\\"id\\\": 1,\\n                \\\"title\\\": \\\"Activity 1\\\",\\n                \\\"due_date\\\": \\\"2025-01-22T21:36:27.6587562+00:00\\\",\\n                \\\"completed\\\": false\\n            },\\n            {\\n                \\\"id\\\": 2,\\n                \\\"title\\\": \\\"Activity 2\\\",\\n                \\\"due_date\\\": \\\"2025-01-22T22:36:27.6587592+00:00\\\",\\n                \\\"completed\\\": true\\n            }\\n        ]\\n    },\\n    \\\"aggregated_data_entity\\\": {\\n        \\\"technical_id\\\": \\\"aggregated_data_entity_001\\\",\\n        \\\"summary\\\": {\\n            \\\"total_activities\\\": 2,\\n            \\\"completed_activities\\\": 1,\\n            \\\"pending_activities\\\": 1\\n        }\\n    },\\n    \\\"report_entity_id\\\": \\\"report_entity_001\\\",\\n    \\\"notification_status\\\": \\\"Sent\\\",\\n    \\\"recipients\\\": [\\n        {\\n            \\\"name\\\": \\\"Admin User\\\",\\n            \\\"email\\\": \\\"admin@example.com\\\"\\n        }\\n    ]\\n}\"}, {\"/tmp/e2ab4ff3-d90d-11ef-abd0-40c2ba0ac9eb/quart-client-template/entity/raw_data_entity/connections/connections_input.md\": \"Curl\\n\\ncurl -X GET \\\"https://fakerestapi.azurewebsites.net/api/v1/Activities\\\" -H  \\\"accept: text/plain; v=1.0\\\"\\nRequest URL\\nhttps://fakerestapi.azurewebsites.net/api/v1/Activities\\nServer response\\nCode\\tDetails\\n200\\t\\nResponse body\\nDownload\\n[\\n  {\\n    \\\"id\\\": 1,\\n    \\\"title\\\": \\\"Activity 1\\\",\\n    \\\"dueDate\\\": \\\"2025-01-22T21:36:27.6587562+00:00\\\",\\n    \\\"completed\\\": false\\n  },\\n  {\\n    \\\"id\\\": 2,\\n    \\\"title\\\": \\\"Activity 2\\\",\\n    \\\"dueDate\\\": \\\"2025-01-22T22:36:27.6587592+00:00\\\",\\n    \\\"completed\\\": true\\n  }]\"}, {\"/tmp/e2ab4ff3-d90d-11ef-abd0-40c2ba0ac9eb/quart-client-template/entity/raw_data_entity/connections/__init__.py\": \"\"}, {\"/tmp/e2ab4ff3-d90d-11ef-abd0-40c2ba0ac9eb/quart-client-template/entity/raw_data_entity/connections/connections.py\": \"# ```python\\nimport asyncio\\nimport logging\\nimport aiohttp\\nfrom common.config.config import ENTITY_VERSION\\nfrom app_init.app_init import entity_service\\n\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\nAPI_URL = \\\"https://fakerestapi.azurewebsites.net/api/v1/Activities\\\"\\n\\nasync def fetch_data():\\n    async with aiohttp.ClientSession() as session:\\n        try:\\n            async with session.get(API_URL, headers={\\\"accept\\\": \\\"text/plain; v=1.0\\\"}) as response:\\n                if response.status == 200:\\n                    return await response.json()\\n                else:\\n                    logger.error(f\\\"Error fetching data: {response.status}\\\")\\n                    return None\\n        except Exception as e:\\n            logger.error(f\\\"Exception occurred: {str(e)}\\\")\\n            return None\\n\\nasync def ingest_data(token: str):\\n    data = await fetch_data()\\n    if data is None:\\n        logger.error(\\\"No data received for ingestion.\\\")\\n        return\\n\\n    # Map raw data to the entity structure\\n    raw_data_entity_data = [\\n        {\\n            \\\"id\\\": activity[\\\"id\\\"],\\n            \\\"title\\\": activity[\\\"title\\\"],\\n            \\\"due_date\\\": activity[\\\"dueDate\\\"],\\n            \\\"completed\\\": activity[\\\"completed\\\"]\\n        } for activity in data\\n    ]\\n\\n    # Save the raw data entity\\n    raw_data_entity_id = await entity_service.add_item(token, \\\"raw_data_entity\\\", ENTITY_VERSION, raw_data_entity_data)\\n    logger.info(f\\\"Raw data entity saved successfully with ID: {raw_data_entity_id}\\\")\\n\\n# Testing with Mocks\\nimport unittest\\nfrom unittest.mock import patch\\n\\nclass TestDataIngestion(unittest.TestCase):\\n\\n    @patch(\\\"aiohttp.ClientSession.get\\\")\\n    @patch(\\\"app_init.app_init.entity_service.add_item\\\")\\n    def test_ingest_data(self, mock_add_item, mock_get):\\n        # Mock the external service response\\n        mock_get.return_value.__aenter__.return_value.status = 200\\n        mock_get.return_value.__aenter__.return_value.json = asyncio.Future()\\n        mock_get.return_value.__aenter__.return_value.json.set_result([\\n            {\\\"id\\\": 1, \\\"title\\\": \\\"Activity 1\\\", \\\"dueDate\\\": \\\"2025-01-22T21:36:27.6587562+00:00\\\", \\\"completed\\\": False},\\n            {\\\"id\\\": 2, \\\"title\\\": \\\"Activity 2\\\", \\\"dueDate\\\": \\\"2025-01-22T22:36:27.6587592+00:00\\\", \\\"completed\\\": True}\\n        ])\\n\\n        mock_add_item.return_value = \\\"raw_data_entity_id\\\"\\n        \\n        # Assuming the token for testing\\n        token = \\\"test_token\\\"\\n        asyncio.run(ingest_data(token))\\n\\n        # Verify that add_item was called with the expected parameters\\n        mock_add_item.assert_called_once_with(\\n            token,\\n            \\\"raw_data_entity\\\",\\n            ENTITY_VERSION,\\n            [\\n                {\\n                    \\\"id\\\": 1,\\n                    \\\"title\\\": \\\"Activity 1\\\",\\n                    \\\"due_date\\\": \\\"2025-01-22T21:36:27.6587562+00:00\\\",\\n                    \\\"completed\\\": False\\n                },\\n                {\\n                    \\\"id\\\": 2,\\n                    \\\"title\\\": \\\"Activity 2\\\",\\n                    \\\"due_date\\\": \\\"2025-01-22T22:36:27.6587592+00:00\\\",\\n                    \\\"completed\\\": True\\n                }\\n            ]\\n        )\\n\\nif __name__ == \\\"__main__\\\":\\n    unittest.main()\\n# ``` \\n# \\n# ### Explanation of the Code\\n# 1. **`fetch_data` Function**: This function makes an asynchronous GET request to the external API to retrieve data. If the request is successful (HTTP status 200), it returns the JSON response.\\n# \\n# 2. **`ingest_data` Function**: \\n#    - This public function first calls `fetch_data` to get the data.\\n#    - If data is received, it maps the raw data to the required entity structure.\\n#    - Finally, it saves the data to the `raw_data_entity` using the `entity_service.add_item` method.\\n# \\n# 3. **Unit Tests**: \\n#    - The tests use the `unittest` module along with `unittest.mock` to simulate external service calls.\\n#    - The `test_ingest_data` method mocks the API response and the `add_item` method to ensure that the `ingest_data` function behaves as expected, verifying that it correctly processes the data and calls the add_item method with the right arguments.\\n# \\n# This setup will allow the user to test the ingestion process in an isolated environment.\"}, {\"/tmp/e2ab4ff3-d90d-11ef-abd0-40c2ba0ac9eb/quart-client-template/entity/raw_data_entity/raw_data_entity.json\": \"[\\n    {\\n        \\\"id\\\": 0,\\n        \\\"title\\\": \\\"string\\\",\\n        \\\"dueDate\\\": \\\"2025-01-22T20:36:27.945Z\\\",\\n        \\\"completed\\\": true\\n    }\\n]\"}, {\"/tmp/e2ab4ff3-d90d-11ef-abd0-40c2ba0ac9eb/quart-client-template/entity/user_requirement.md\": \"## Detailed Summary of User Requirement\\n\\nThe user has expressed a need to develop an application focusing on efficient data processing. The application must encompass several key functionalities, which are outlined below:\\n\\n### 1. Data Ingestion\\n- **Requirement**: The application should be capable of ingesting data from a specified data source.\\n- **Capabilities**:\\n  - While the user has not specified the exact source, the application should support various types of input, including external APIs, databases, or user submissions.\\n  - The data ingestion process needs to be automated and scheduled to run **once a day** at a specific time.\\n  \\n### 2. Data Aggregation\\n- **Requirement**: Once the data is ingested, the application must aggregate this data.\\n- **Capabilities**:\\n  - The aggregation process involves transforming the raw data into a summarized form.\\n  - This may include calculations, grouping, or other forms of data manipulation to extract meaningful insights from the ingested data.\\n\\n### 3. Report Generation\\n- **Requirement**: After the aggregation of data, the application should save the aggregated information into a report format.\\n- **Capabilities**:\\n  - The report must be structured and easily interpretable, as it is intended to be shared with stakeholders.\\n  \\n### 4. Email Notification\\n- **Requirement**: Upon the successful generation of the report, the application must automatically send the report to the admin's email.\\n- **Capabilities**:\\n  - The email notification should include relevant information about the report and be formatted appropriately for easy reading.\\n  \\n### 5. Scheduling Mechanism\\n- **Requirement**: The application should implement a scheduling mechanism to ensure that the data ingestion job runs reliably every day.\\n- **Capabilities**:\\n  - While the user has not specified the technology for scheduling, it could involve cron jobs, built-in scheduling functionalities, or external scheduling services to manage the timing of the data ingestion process.\\n\\n### 6. Admin Interaction\\n- **Requirement**: The admin will be the primary recipient of the reports generated by the application.\\n- **Capabilities**:\\n  - The application should not only send reports but also ensure that the email is sent without errors.\\n  - There must be a notification mechanism in place to inform the admin in case of failures during the report generation or email dispatch.\\n\\n### Additional Considerations\\n- The user has not indicated specific technologies or tools for implementation, allowing for flexibility in choosing appropriate stacks.\\n- The design should maintain a clear audit trail:\\n  - This may involve logging actions taken during data ingestion, aggregation, and report generation for accountability and traceability.\\n- Error handling must be incorporated to manage potential failures during data ingestion, report generation, or email sending.\\n\\n### Overall Goal\\nThe overall goal of the application is to streamline data ingestion, aggregation, and reporting processes while providing timely notifications to the admin. This will enhance efficiency and offer a user-friendly experience, ultimately supporting stakeholders with actionable insights derived from the data.\"}] . Do not do any mapping logic - it is not relevant. Just remember the code and the application design to reuse in your future application building. Return confirmation that you remembered everything",
        "answer": "I have successfully remembered the contents of the specified files, including the `entity_service_interface.py`, `trino_service.py`, `ai_assistant_service.py`, and the processor instructions. I will be able to reuse this information for future application building. If you need any further assistance or have specific requests, please let me know!"
    },
    {
        "ai_endpoint": "cyoda",
        "question": "Please, generate the processor functions for data_ingestion_job call public functions by the name of each processor: ingest_raw_data, aggregate_raw_data_process, generate_report_process.  Reuse functions that are available in the code base, including logic.app_init import entity_service, connections.py (ingest_data public function) and any other existing function that is related to your purpose. Make sure you include logic to save any dependant entities: raw_data_entity, aggregated_data_entity, report_entity. Also generate tests with mocks for external services or functions so that the user can try out the functions right away in isolated environment. **Tests should be in the same file with the code**data_ingestion_job. Based on the user suggestions if there are any.  Please make sure you are re-using all raw_data_*/connections/connection.py ingest_data functions. This is very important not to re-implement ingest_data but reuse it. You should import and reuse all ingest_data functions, use 'as' to avoid names duplicates. Make sure the result of data ingestion is saved to the corresponding raw data entity. Please also make sure that you understand that argument 'data' that you pass to each function corresponds to entity/data_ingestion_job/data_ingestion_job.json data and not to any other entity! User says: : What would you recommend?",
        "answer": "Here's the implementation of the processor functions for the `data_ingestion_job`. These functions will include `ingest_raw_data`, `aggregate_raw_data_process`, and `generate_report_process`. I will reuse the existing `ingest_data` function defined in `raw_data_entity/connections/connections.py` and ensure that the outputs are saved to their respective entities.\n\n```python import logging\\nimport asyncio\\nfrom app_init.app_init import entity_service\\nfrom entity.raw_data_entity.connections.connections import ingest_data as ingest_raw_data_connection\\nfrom common.config.config import ENTITY_VERSION\\n\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\n\\nasync def ingest_raw_data(meta, data):\\n    logger.info(\"Starting data ingestion process.\")\\n    try:\\n        # Call the reusable ingest_data function\\n        raw_data = await ingest_raw_data_connection()\\n\\n        # Save the raw data entity\\n        raw_data_entity_id = await entity_service.add_item(\\n            meta[\"token\"], \"raw_data_entity\", ENTITY_VERSION, raw_data\\n        )\\n\\n        # Update the data with the raw data entity ID\\n        data[\"raw_data_entity\"] = {\"technical_id\": raw_data_entity_id, \"records\": raw_data}\\n        logger.info(f\"Raw data entity saved successfully with ID: {raw_data_entity_id}\")\\n    except Exception as e:\\n        logger.error(f\"Error in ingest_raw_data: {e}\")\\n        raise\\n\\n\\nasync def aggregate_raw_data_process(meta, data):\\n    logger.info(\"Starting data aggregation process.\")\\n    try:\\n        # Simulate aggregation logic\\n        activities = data[\"raw_data_entity\"][\"records\"]\\n        aggregated_data = {\\n            \"total_activities\": len(activities),\\n            \"completed_activities\": sum(1 for activity in activities if activity.get(\"completed\")),\\n            \"pending_activities\": sum(1 for activity in activities if not activity.get(\"completed\")),\\n            \"activity_summary\": activities,\\n        }\\n\\n        # Save the aggregated data entity\\n        aggregated_data_entity_id = await entity_service.add_item(\\n            meta[\"token\"], \"aggregated_data_entity\", ENTITY_VERSION, aggregated_data\\n        )\\n\\n        # Update the data with the aggregated data entity ID\\n        data[\"aggregated_data_entity\"] = {\"technical_id\": aggregated_data_entity_id}\\n        logger.info(f\"Aggregated data entity saved successfully with ID: {aggregated_data_entity_id}\")\\n    except Exception as e:\\n        logger.error(f\"Error in aggregate_raw_data_process: {e}\")\\n        raise\\n\\n\\nasync def generate_report_process(meta, data):\\n    logger.info(\"Starting report generation process.\")\\n    try:\\n        report_data = {\\n            \"report_id\": \"report_001\",\\n            \"generated_at\": \"2023-10-01T10:00:00Z\",\\n            \"report_title\": \"Monthly Data Analysis\",\\n            \"summary\": data[\"aggregated_data_entity\"][\"summary\"],  # Using the aggregated data\\n            \"distribution_info\": {},\\n        }\\n\\n        # Save the report entity\\n        report_entity_id = await entity_service.add_item(\\n            meta[\"token\"], \"report_entity\", ENTITY_VERSION, report_data\\n        )\\n\\n        # Log the report generation\\n        logger.info(f\"Report entity saved successfully with ID: {report_entity_id}\")\\n    except Exception as e:\\n        logger.error(f\"Error in generate_report_process: {e}\")\\n        raise\\n\\n\\n# Testing with Mocks\\nimport unittest\\nfrom unittest.mock import patch\\n\\n\\nclass TestDataIngestionJob(unittest.TestCase):\\n\\n    @patch(\"workflow.ingest_raw_data_connection\")\\n    @patch(\"app_init.app_init.entity_service.add_item\")\\n    def test_ingest_raw_data(self, mock_add_item, mock_ingest_data):\\n        mock_ingest_data.return_value = [{\"id\": 1, \"title\": \"Activity 1\", \"due_date\": \"2025-01-22\", \"completed\": False}]\\n        mock_add_item.return_value = \"raw_data_entity_id\"\\n\\n        meta = {\"token\": \"test_token\"}\\n        data = {}\\n\\n        asyncio.run(ingest_raw_data(meta, data))\\n\\n        mock_add_item.assert_called_once_with(\\n            meta[\"token\"], \"raw_data_entity\", ENTITY_VERSION,\\n            [{\"id\": 1, \"title\": \"Activity 1\", \"due_date\": \"2025-01-22\", \"completed\": False}]\\n        )\\n\\n    @patch(\"app_init.app_init.entity_service.add_item\")\\n    def test_aggregate_raw_data_process(self, mock_add_item):\\n        mock_add_item.return_value = \"aggregated_data_entity_id\"\\n\\n        meta = {\"token\": \"test_token\"}\\n        data = {\\n            \"raw_data_entity\": {\\n                \"technical_id\": \"raw_data_entity_001\",\\n                \"records\": [\\n                    {\\n                        \"id\": 1,\\n                        \"title\": \"Activity 1\",\\n                        \"due_date\": \"2025-01-22T21:36:27.6587562+00:00\",\\n                        \"completed\": False\\n                    },\\n                    {\\n                        \"id\": 2,\\n                        \"title\": \"Activity 2\",\\n                        \"due_date\": \"2025-01-22T22:36:27.6587592+00:00\",\\n                        \"completed\": True\\n                    }\\n                ]\\n            }\\n        }\\n\\n        asyncio.run(aggregate_raw_data_process(meta, data))\\n\\n        # Ensure the data counts are correct:\\n        total_activities = len(data[\"raw_data_entity\"][\"records\"])\\n        completed_activities = sum(1 for record in data[\"raw_data_entity\"][\"records\"] if record[\"completed\"])\\n        pending_activities = total_activities - completed_activities\\n\\n        mock_add_item.assert_called_once_with(\\n            meta[\"token\"], \"aggregated_data_entity\", ENTITY_VERSION,\\n            {\\n                \"total_activities\": total_activities,\\n                \"completed_activities\": completed_activities,\\n                \"pending_activities\": pending_activities,\\n                \"activity_summary\": data[\"raw_data_entity\"][\"records\"],\\n            }\\n        )\\n\\n    @patch(\"app_init.app_init.entity_service.add_item\")\\n    def test_generate_report_process(self, mock_add_item):\\n        mock_add_item.return_value = \"report_entity_id\"\\n\\n        meta = {\"token\": \"test_token\"}\\n        data = {\\n            \"raw_data_entity\": {\\n                \"technical_id\": \"raw_data_entity_001\",\\n                \"records\": [\\n                    {\\n                        \"id\": 1,\\n                        \"title\": \"Activity 1\",\\n                        \"due_date\": \"2025-01-22T21:36:27.6587562+00:00\",\\n                        \"completed\": False\\n                    },\\n                    {\\n                        \"id\": 2,\\n                        \"title\": \"Activity 2\",\\n                        \"due_date\": \"2025-01-22T22:36:27.6587592+00:00\",\\n                        \"completed\": True\\n                    }\\n                ]\\n            },\\n            \"aggregated_data_entity\": {\\n                \"technical_id\": \"aggregated_data_entity_001\",\\n                \"summary\": {\\n                    \"total_activities\": 2,\\n                    \"completed_activities\": 1,\\n                    \"pending_activities\": 1\\n                }\\n            }\\n        }\\n\\n\\n        asyncio.run(generate_report_process(meta, data))\\n\\n        mock_add_item.assert_called_once_with(\\n        meta[\"token\"], \"report_entity\", ENTITY_VERSION,\\n        {\\n            \"report_id\": \"report_001\",\\n            \"generated_at\": \"2023-10-01T10:00:00Z\",\\n            \"report_title\": \"Monthly Data Analysis\",\\n            \"summary\": data[\"aggregated_data_entity\"][\"summary\"],\\n            \"distribution_info\": {},\\n        }\\n\\n    )\\n\\n\\nif __name__ == \"__main__\":\\n    unittest.main() ```\n\n### Explanation of the Code:\n1. **Ingest Raw Data**:\n   - The `ingest_raw_data` function calls the `ingest_data` function to fetch raw data, saves it to the `raw_data_entity`, and updates the `data` object with the technical ID of the saved entity.\n\n2. **Aggregate Raw Data**:\n   - The `aggregate_raw_data_process` function processes the ingested data and aggregates it, saving the aggregated data to `aggregated_data_entity`.\n\n3. **Generate Report**:\n   - The `generate_report_process` function creates a report based on the aggregated data and saves it to `report_entity`.\n\n4. **Unit Tests**:\n   - Each function includes unittest cases that mock external service calls to test the functionality of the processor functions in isolation. This allows for testing the logic without relying on actual service implementations. \n\nThis setup ensures that the functions can be tested and reused effectively in future application development."
    },