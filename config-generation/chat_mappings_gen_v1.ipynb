{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Cyoda configurations Q&A with RAG Langchain\n",
    "\n",
    "This is a playground for experimenting with mappings generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "WORK_DIR = os.environ[\"WORK_DIR\"]\n",
    "API_KEY = os.environ[\"CYODA_API_KEY\"]\n",
    "API_SECRET = os.environ[\"CYODA_API_SECRET\"]\n",
    "API_URL = os.environ[\"CYODA_API_URL\"] + \"/api\"\n",
    "GRPC_ADDRESS = os.environ[\"GRPC_ADDRESS\"]\n",
    "WORK_DIR = os.environ[\"WORK_DIR\"]\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "WORK_DIR = os.environ[\"WORK_DIR\"]\n",
    "TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "##for google colab (optional)\n",
    "# This cell is optional and can be skipped\n",
    "from google.colab import userdata\n",
    "API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "WORK_DIR = userdata.get('WORK_DIR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle unsupported version of sqlite3 (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pysqlite3-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "__import__(\"pysqlite3\")\n",
    "sys.modules[\"sqlite3\"] = sys.modules[\"pysqlite3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import GitLoader, DirectoryLoader, TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    temperature=0.45,\n",
    "    max_tokens=4000,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load instructions and entities from the official cyoda repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "loader = GitLoader(\n",
    "    repo_path=WORK_DIR,\n",
    "    branch=\"develop\",\n",
    "    file_filter=lambda file_path: file_path.startswith(f\"{WORK_DIR}/data/rag/v1/connections/templates\"),\n",
    ")\n",
    "docs = loader.load()\n",
    "print(f\"Number of documents loaded: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(f\"{WORK_DIR}/data/rag/v1/mappings\", loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "print(f\"Number of documents loaded: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split documents and create vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = vectorstore._collection.count()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = vectorstore.similarity_search(\"Get some document\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define prompts for contextualizing question and answering question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are a mapping generation code assistant assistant. \\\n",
    "You are an expert in Javascript Nashorn and understand how it is different from Java and javascript.\n",
    "You will be asked to generate Nashorn javascript code to map input to entity. \\\n",
    "First, analyse the input and the entity and fill in Mapping Questionnaire.\n",
    "Then do your best to do code assistance for mapping the input to the entity.   \n",
    "{context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize chat history and relevant functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add a message to the chat history\n",
    "def add_to_chat_history(id, question, message):\n",
    "    if id in chat_history:\n",
    "        chat_history[id].extend([HumanMessage(content=question), message])\n",
    "    else:\n",
    "        chat_history[id] = [HumanMessage(content=question), message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clear chat history\n",
    "def clear_chat_history(id):\n",
    "    if id in chat_history:\n",
    "        del chat_history[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(id, question):\n",
    "    ai_msg = rag_chain.invoke(\n",
    "        {\"input\": question, \"chat_history\": chat_history.get(id, [])}\n",
    "    )\n",
    "    add_to_chat_history(id, question, ai_msg[\"answer\"])\n",
    "    return ai_msg[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a chat session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Generate a unique ID for the chat session\n",
    "id = uuid.uuid1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear chat history if necessary\n",
    "clear_chat_history(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(result):\n",
    "    if result.startswith(\"```\"):\n",
    "        return \"\\n\".join(result.split(\"\\n\")[1:-1])\n",
    "    if not result.startswith(\"{\"):\n",
    "        start_index = result.find(\"```json\")\n",
    "        if start_index != -1:\n",
    "            start_index += len(\"```json\\n\")\n",
    "            end_index = result.find(\"```\", start_index)\n",
    "            return result[start_index:end_index].strip()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uuid() -> uuid:\n",
    "    return uuid.uuid1()\n",
    "\n",
    "generate_uuid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonschema\n",
    "from jsonschema import validate\n",
    "\n",
    "# connections_file_path = f'{WORK_DIR}/data/v1/connections/connection_json_schema.json'\n",
    "# endpoints_file_path = f'{WORK_DIR}/data/v1/connections/endpoint_json_schema.json'\n",
    "\n",
    "\n",
    "def validate_result(parsed_result: str, file_path: str) -> bool:\n",
    "    with open(file_path, \"r\") as schema_file:\n",
    "        schema = json.load(schema_file)\n",
    "\n",
    "    json_data = json.loads(parsed_result)\n",
    "\n",
    "    try:\n",
    "        validate(instance=json_data, schema=schema)\n",
    "        print(\"JSON is valid.\")\n",
    "        return True\n",
    "    except jsonschema.exceptions.ValidationError as err:\n",
    "        print(\"JSON is invalid:\", err.message)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(file_path):\n",
    "    data = \"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            data = file.read()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read JSON file: {e}\")\n",
    "        return\n",
    "    return data\n",
    "\n",
    "\n",
    "data = get_input(f\"{WORK_DIR}/data/test-inputs/v1/mappings/tender_level_0.json\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = get_input(f\"{WORK_DIR}/data/test-inputs/v1/mappings/tender_level_2.json\")\n",
    "entity_name = \"Tender Entity f2c3867f-6ddc-4a48-a47a-03ea7ac6b306\"\n",
    "question = f\"Get {entity_name} entity json schema from the context. If you don't have it - return that you do not have data for {entity_name} entity and stop execution. Fill in Mappings Questionnaire json based on the input: {input}. Return the resulting Questionnaire json.\"\n",
    "questionnaire_result = ask_question(id, question)\n",
    "print(questionnaire_result)\n",
    "try:\n",
    "    parsed_questionnaire_result = parse_json(questionnaire_result)\n",
    "    print(parsed_questionnaire_result)\n",
    "    parsed_questionnaire_result_json = json.loads(parsed_questionnaire_result)\n",
    "except Exception as e:\n",
    "    print(\"error\")\n",
    "    print(parsed_questionnaire_result)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = f\"Write code to map {entity_name} to the provided input. Base your answer on the available list_of_input_to_entity_properties\"\n",
    "questionnaire_result = ask_question(id, question)\n",
    "print(questionnaire_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = f\"Write code to filter notices by noticeTitle. Exclude all notices that start with P. Return only this piece of code\"\n",
    "questionnaire_result = ask_question(id, question)\n",
    "print(questionnaire_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paths(data, current_path=\"\"):\n",
    "    paths = []\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            new_path = f\"{current_path}/{key}\" if current_path else key\n",
    "            if isinstance(value, (dict, list)):\n",
    "                paths.extend(generate_paths(value, new_path))\n",
    "            else:\n",
    "                paths.append(new_path)\n",
    "    elif isinstance(data, list):\n",
    "        for i in range(len(data)):\n",
    "            new_path = f\"{current_path}/*\"\n",
    "            if isinstance(data[i], (dict, list)):\n",
    "                paths.extend(generate_paths(data[i], new_path))\n",
    "            else:\n",
    "                paths.append(new_path)\n",
    "\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(input))\n",
    "input_json = json.loads(input)\n",
    "print(type(input_json))\n",
    "paths = generate_paths(input_json)\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = {\"script\": {\"body\": str(parsed_questionnaire_result), \"inputSrcPaths\": str(paths)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear chat history if necessary\n",
    "clear_chat_history(id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
