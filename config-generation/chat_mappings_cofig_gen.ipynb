{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Cyoda configurations Q&A with RAG Langchain\n",
    "\n",
    "Welcome to this Jupyter notebook! This notebook serves as your guide to developing an AI-powered Question & Answer system using the Langchain library. This system utilizes the Retrieval-Augmented Generation (RAG) model that leverages OpenAI's GPT-3 model to provide intelligent and context-aware responses.\n",
    "\n",
    "The primary purpose of this notebook is to generate Cyoda mapping configurations and resources. It does so by interacting with the data set available in the official Cyoda repository.\n",
    "\n",
    "## What will we cover?\n",
    "\n",
    "In this notebook, we will go through the following steps:\n",
    "\n",
    "1. **Setting up the environment**: We will install necessary libraries and load environment variables.\n",
    "\n",
    "2. **Initializing the AI model**: We will initialize the ChatOpenAI model with the appropriate parameters.\n",
    "\n",
    "3. **Loading instructions and entities**: We will load instructions and entities from the official repository using the GitLoader.\n",
    "\n",
    "4. **Splitting documents and creating a vectorstore**: We will split the loaded documents into chunks and create a vectorstore using the Chroma library.\n",
    "\n",
    "5. **Defining prompts for contextualizing and answering questions**: We will define prompts that the AI model will use to contextualize and answer questions.\n",
    "\n",
    "6. **Creating a retrieval chain**: We will create a retrieval chain that combines the history-aware retriever and the question-answer chain.\n",
    "\n",
    "7. **Running the chatbot**: Finally, we will run the chatbot and see it in action!\n",
    "\n",
    "## Let's get started!\n",
    "\n",
    "Please follow along with the code cells and comments to understand each step of the process. If you have any questions or run into any issues, feel free to ask for help. Happy coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "WORK_DIR = os.environ['WORK_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "##for google colab (optional)\n",
    "# This cell is optional and can be skipped\n",
    "from google.colab import userdata\n",
    "API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "WORK_DIR = userdata.get('WORK_DIR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle unsupported version of sqlite3 (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "pip install pysqlite3-binary==0.5.2.post3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "import sys\n",
    "__import__('pysqlite3')\n",
    "sys.modules['sqlite3'] = sys.modules[\"pysqlite3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import GitLoader, WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, max_tokens = 6000, model=\"gpt-3.5-turbo-16k\", openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load instructions and entities from the official cyoda repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/Cyoda-platform/cyoda-ai\",\n",
    "    repo_path=WORK_DIR,\n",
    "    branch=\"cyoda-ai-configurations-3.0.x\",\n",
    "    file_filter=lambda file_path: file_path.startswith(\n",
    "        f\"{WORK_DIR}/data/config-generation/mappings/\"\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "print(f\"Number of documents loaded: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded: 0\n"
     ]
    }
   ],
   "source": [
    "#alternatively specify only repo_path without clone_url for a local repository\n",
    "loader = GitLoader(\n",
    "    repo_path=WORK_DIR,\n",
    "    branch=\"cyoda-ai-configurations-3.0.x\",\n",
    "    file_filter=lambda file_path: file_path.startswith(\n",
    "        f\"{WORK_DIR}/data/config-generation/mappings/input_scrc_paths_instruction_0.txt\"\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "print(f\"Number of documents loaded: {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kseniia/Documents/python/cyoda-ai\n"
     ]
    }
   ],
   "source": [
    "print(WORK_DIR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader([\"https://docs.oracle.com/javase/8/docs/technotes/guides/scripting/prog_guide/javascript.html\"])\n",
    "scripting_docs = loader.load()\n",
    "docs.extend(scripting_docs)\n",
    "print(f\"Number of documents loaded: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split documents and create vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = vectorstore._collection.count()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define prompts for contextualizing question and answering question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are a mapping tool. You should do your best to answer the question.\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add a message to the chat history\n",
    "def add_to_chat_history(id, question, message):\n",
    "    if id in chat_history:\n",
    "        chat_history[id].extend([HumanMessage(content=question), message])\n",
    "    else:\n",
    "        chat_history[id] = [HumanMessage(content=question), message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clear chat history\n",
    "def clear_chat_history(id):\n",
    "    if id in chat_history:\n",
    "        del chat_history[id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read file content\n",
    "def read_file_to_string(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = read_file_to_string(f\"{WORK_DIR}/data/entities/tender_entity/resources/data_source_inputs/tender_input_1.json\")\n",
    "ENTITY = \"net.cyoda.saas.model.TenderEntity\"\n",
    "RETURN_STRING = \"Return only DataMappingConfigDto json.\"\n",
    "question = f\"Produce a mapping from this input to this target entity. Input: {INPUT}. Entity: {ENTITY}. {RETURN_STRING}. Work only with JSON attributes that are simple and not arrays. Do NOT use any atributes that are not present in schema. {RETURN_STRING}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Generate a unique ID for the chat session\n",
    "id = uuid.uuid1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First question and AI response\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history.get(id, [])})\n",
    "add_to_chat_history(id, question, ai_msg_1[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ai_msg_1[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "second_question = f\"Produce a script for this mapping. Include more complex attributes of the {ENTITY} schema, like arrays if applicable. Return only script json object which contains body and inputSrcPaths inside script attribute.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second question and AI response\n",
    "second_question = f\"Write a JavaScript Nashorn script to map the given input to given {ENTITY}. Include more complex attributes of the {ENTITY} schema, like arrays if applicable. Check in the instruction how to write inputSrcPaths for nested objects. Return only script json object which contains body and inputSrcPaths inside script attribute.\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history.get(id, [])})\n",
    "add_to_chat_history(id, second_question, ai_msg_2[\"answer\"])\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second question and AI response\n",
    "second_question = f\"Can you show me the place in the instruction which explains inputSrcPaths for nested objects\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history.get(id, [])})\n",
    "add_to_chat_history(id, second_question, ai_msg_2[\"answer\"])\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_chat_history(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ai_msg_1[\"context\"][0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
