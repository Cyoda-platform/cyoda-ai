{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AI Cyoda configurations Q&A with RAG Langchain\n",
                "\n",
                "Welcome to this Jupyter notebook! This notebook serves as your guide to developing an AI-powered Question & Answer system using the Langchain library. This system utilizes the Retrieval-Augmented Generation (RAG) model, a powerful tool that leverages OpenAI's GPT-3 model to provide intelligent and context-aware responses.\n",
                "\n",
                "The primary purpose of this notebook is to generate Cyoda mapping configurations and resources. It does so by interacting with the data set available in the official Cyoda repository. By following along, you'll learn how to harness the power of Langchain and RAG to create a sophisticated AI tool for Cyoda.\n",
                "\n",
                "## What will we cover?\n",
                "\n",
                "In this notebook, we will go through the following steps:\n",
                "\n",
                "1. **Setting up the environment**: We will install necessary libraries and load environment variables.\n",
                "\n",
                "2. **Initializing the AI model**: We will initialize the ChatOpenAI model with the appropriate parameters.\n",
                "\n",
                "3. **Loading instructions and entities**: We will load instructions and entities from the official repository using the GitLoader.\n",
                "\n",
                "4. **Splitting documents and creating a vectorstore**: We will split the loaded documents into chunks and create a vectorstore using the Chroma library.\n",
                "\n",
                "5. **Defining prompts for contextualizing and answering questions**: We will define prompts that the AI model will use to contextualize and answer questions.\n",
                "\n",
                "6. **Creating a retrieval chain**: We will create a retrieval chain that combines the history-aware retriever and the question-answer chain.\n",
                "\n",
                "7. **Running the chatbot**: Finally, we will run the chatbot and see it in action!\n",
                "\n",
                "## Let's get started!\n",
                "\n",
                "Please follow along with the code cells and comments to understand each step of the process. If you have any questions or run into any issues, feel free to ask for help. Happy coding!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Install requirements"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pip install -r ../requirements.txt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Load environment variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from dotenv import load_dotenv\n",
                "import os\n",
                "\n",
                "load_dotenv()\n",
                "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "##for google colab (optional)\n",
                "# This cell is optional and can be skipped\n",
                "#from google.colab import userdata\n",
                "#API_KEY = userdata.get('OPENAI_API_KEY')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Handle unsupported version of sqlite3 (optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "__import__('pysqlite3')\n",
                "sys.modules['sqlite3'] = sys.modules[\"pysqlite3\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "from langchain_openai import OpenAIEmbeddings\n",
                "from langchain_openai import ChatOpenAI\n",
                "from langchain_community.document_loaders import GitLoader\n",
                "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
                "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
                "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
                "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "from langchain_core.messages import HumanMessage\n",
                "from langchain_community.vectorstores import Chroma"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Initialize ChatOpenAI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "llm = ChatOpenAI(temperature=0.7, max_tokens = 6000, model=\"gpt-3.5-turbo-16k\", openai_api_key=OPENAI_API_KEY)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Load instructions and entities from the official cyoda repository"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "loader = GitLoader(\n",
                "    clone_url=\"https://github.com/Cyoda-platform/cyoda-ai\",\n",
                "    repo_path=\"./data/config-generation/\",\n",
                "    branch=\"cyoda-ai-configurations-3.0.x\",\n",
                ")\n",
                "docs = loader.load()\n",
                "print(f\"Number of documents loaded: {len(docs)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Split documents and create vectorstore"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
                "splits = text_splitter.split_documents(docs)\n",
                "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
                "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "count = vectorstore._collection.count()\n",
                "print(count)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Define prompts for contextualizing question and answering question"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
                "which might reference context in the chat history, formulate a standalone question \\\n",
                "which can be understood without the chat history. Do NOT answer the question, \\\n",
                "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
                "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
                "    [\n",
                "        (\"system\", contextualize_q_system_prompt),\n",
                "        MessagesPlaceholder(\"chat_history\"),\n",
                "        (\"human\", \"{input}\"),\n",
                "    ]\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "history_aware_retriever = create_history_aware_retriever(\n",
                "    llm, retriever, contextualize_q_prompt\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Answer question"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "qa_system_prompt = \"\"\"You are a mapping tool. You should do your best to answer the question.\n",
                "Use the following pieces of retrieved context to answer the question. \\\n",
                "\n",
                "{context}\"\"\"\n",
                "qa_prompt = ChatPromptTemplate.from_messages(\n",
                "    [\n",
                "        (\"system\", qa_system_prompt),\n",
                "        MessagesPlaceholder(\"chat_history\"),\n",
                "        (\"human\", \"{input}\"),\n",
                "    ]\n",
                ")\n",
                "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Create retrieval chain"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to read file content\n",
                "def read_file_to_string(file_path):\n",
                "    with open(file_path, 'r') as file:\n",
                "        return file.read()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Define question"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "INPUT = read_file_to_string(\"../data/entities/tender_entity/resources/data_source_inputs/tender_input_1.json\")\n",
                "ENTITY = \"tender_entity\"\n",
                "RETURN_STRING = \"Return only DataMappingConfigDto json.\"\n",
                "question = f\"Produce a mapping from this input to this target entity. Input: {INPUT}. Entity: {ENTITY}. {RETURN_STRING}\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Initialize chat history"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "chat_history = {}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to add a message to the chat history\n",
                "def add_to_chat_history(id, question, message):\n",
                "    if id in chat_history:\n",
                "        chat_history[id].extend([HumanMessage(content=question), message])\n",
                "    else:\n",
                "        chat_history[id] = [HumanMessage(content=question), message]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to clear chat history\n",
                "def clear_chat_history(id):\n",
                "    if id in chat_history:\n",
                "        del chat_history[id]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import uuid\n",
                "\n",
                "# Generate a unique ID for the chat session\n",
                "id = uuid.uuid1()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# First question and AI response\n",
                "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history.get(id, [])})\n",
                "add_to_chat_history(id, question, ai_msg_1[\"answer\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(ai_msg_1[\"answer\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Second question and AI response\n",
                "second_question = \"Produce a script for this mapping. Return only script json object\"\n",
                "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history.get(id, [])})\n",
                "add_to_chat_history(id, second_question, ai_msg_2[\"answer\"])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Print the AI's response to the second question\n",
                "print(ai_msg_2[\"answer\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(chat_history)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for document in ai_msg_1[\"context\"]:\n",
                "    print(document)\n",
                "    print()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
